{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folder Path and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#CONST\n",
    "\n",
    "folder_path = r\"D:\\data\\CGIAR\\UNFCCC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adaptation Fund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the project number to the original CSV file downloaded from the 'adaptation_fund_path' website. Also, identify projects listed on the website that have no corresponding documents in 'AF_projects_not_in_documents'. Date files indicate when the data collection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'C:\\Users\\david\\My Drive\\data\\analysis_git_data\\cgiar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m projects_not_in_documents \u001b[38;5;241m=\u001b[39m df_adaptation_fund[\u001b[38;5;241m~\u001b[39mdf_adaptation_fund[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprojecturl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(df_project_documents[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject_link\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[0;32m     25\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAF_projects_not_in_documents_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mprojects_not_in_documents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdavid\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMy Drive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43manalysis_git_data\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcgiar\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# drop if no project number\u001b[39;00m\n\u001b[0;32m     30\u001b[0m df_joined_notna \u001b[38;5;241m=\u001b[39m df_joined[df_joined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject_no\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:3964\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3953\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3955\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3956\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3957\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3961\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3962\u001b[0m )\n\u001b[1;32m-> 3964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'C:\\Users\\david\\My Drive\\data\\analysis_git_data\\cgiar'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Paths to your CSV files\n",
    "\n",
    "adaptation_fund_path = os.path.join(folder_path, \"processing_stage_movebackmain\", \"AF_20240403.csv\")  #the file downloaded as csv from ADF\n",
    "project_documents_path = os.path.join(folder_path, \"processing_stage_movebackmain\", \"AF_project_documents_links_with_titles_unique.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV files\n",
    "df_adaptation_fund = pd.read_csv(adaptation_fund_path, on_bad_lines='skip',encoding='utf-8')\n",
    "df_project_documents = pd.read_csv(project_documents_path, on_bad_lines='skip',encoding='utf-8')\n",
    "\n",
    "# Ensure 'project_link' is in df_project_documents to proceed with the merge\n",
    "df_joined = pd.merge(df_adaptation_fund, df_project_documents[['project_link', 'project_title', 'document_link', 'project_no']], left_on='projecturl', right_on='project_link', how='left')\n",
    "\n",
    "    # Remove the 'project_link' column\n",
    "df_joined.drop('project_link', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "#find if projects no documents\n",
    "projects_not_in_documents = df_adaptation_fund[~df_adaptation_fund['projecturl'].isin(df_project_documents['project_link'])]\n",
    "filename = f\"AF_projects_not_in_documents_{current_time}.csv\"\n",
    "projects_not_in_documents.to_csv(rf\"C:\\Users\\david\\My Drive\\data\\analysis_git_data\\cgiar\\{filename}\", index=False)\n",
    "\n",
    "# drop if no project number\n",
    "\n",
    "df_joined_notna = df_joined[df_joined['project_no'].notnull()]\n",
    "#add org column for master file\n",
    "df_joined_notna['org'] = 'AF'\n",
    "df_joined_notna.rename(columns={'projecturl': 'url', 'grantamount':'grant_amount', 'projecttitle': 'title', 'ie':'entity', 'fy_approval':'year'}, inplace=True) #rename for consistency with master // TODO:check with Cesare if BM is status.  \n",
    "\n",
    "df_joined_notna['theme'] = 'Adaptation'\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "filename = f\"AF_joined_projects_{current_time}.csv\"\n",
    "\n",
    "full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "df_joined_notna.to_csv(full_path, index=False,encoding='utf-8')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. GCF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gcf_projects = pd.read_csv(os.path.join(folder_path, \"processing_stage_movebackmain\", \"gcf_projects_20240403.csv\"),encoding='utf-8') #the file downloaded as csv from GCF\n",
    "gcf_projects['org'] = 'GCF'\n",
    "gcf_projects.rename(columns={'Ref #': 'project_no', 'Countries': 'country', 'Project Name': 'title', 'Sector': 'sector', 'Entity': 'entity', 'BM': 'year', 'FA Financing': 'grant_amount', 'Theme': 'theme'}, inplace=True) #rename for consistency with master // TODO:check with Cesare if BM is status.  \n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "filename = f\"GCF_joined_projects_{current_time}.csv\"\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "gcf_projects.to_csv(file_path, index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gef_df = pd.read_csv(os.path.join(folder_path, \"processing_stage_movebackmain\", \"gef_projects.csv\"),encoding='utf-8') #the file downloaded as csv from GEF with filter climate change\n",
    "gef_df['org'] = 'GEF'\n",
    "gef_df.rename(columns={'ID': 'project_no', 'Countries': 'country', 'Title': 'title', 'Focal Areas': 'sector', 'Agencies': 'entity', 'GEF Grant': 'grant_amount', 'Status':'status', 'Approval FY': 'year'}, inplace=True) #rename for consistency with master\n",
    "\n",
    "base_url = 'https://www.thegef.org/projects-operations/projects/'\n",
    "\n",
    "gef_df['url'] = gef_df['project_no'].apply(lambda x: base_url + str(x))\n",
    "\n",
    "\n",
    "gef_df['theme'] = 'NA'\n",
    "\n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "filename = f\"GEF_joined_projects_{current_time}.csv\"\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "gef_df.to_csv(file_path, index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the Master file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'Unspecified' in 'country_iso3': 139\n",
      "Count of 'Unspecified' in 'country_iso3': 114\n",
      "Total projects from the Adaptation Fund are 147. On the website, there are 149. Missing or not in dataset: 2.\n",
      "Total projects from the GCF are 253. On the website, there are 253. Missing or not in dataset: 0.\n",
      "Total projects from the GEF are 2402. On the website, there are 2402. Missing or not in dataset: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19372\\1467963200.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['country_iso3'] = filtered_df['title'].apply(extract_countries_to_iso3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pycountry\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Load the CSV file with UTF-8 encoding\n",
    "data = pd.read_csv(r\"D:\\data\\CGIAR\\UNFCCC\\processing_stage_movebackmain\\master_file_af_gcf_gef_20240517.csv\", encoding='utf-8')\n",
    "\n",
    "# Extended mapping for normalizing country names and their ISO3 codes\n",
    "country_normalization_mapping = {\n",
    "    'Libyan Arab Jamahiriya': 'Libya',\n",
    "    'Côte d’Ivoire': 'CIV',\n",
    "    'Cote d\\'Ivoire': 'CIV',\n",
    "    'C&ocirc;te d\\'Ivoire': 'CIV',\n",
    "    'Micronesia, Federated States of': 'FSM',\n",
    "    'Micronesia (Federated States of)': 'FSM',\n",
    "    'Micronesia': 'FSM',\n",
    "    'Comoros (the)': 'Comoros',\n",
    "    'Democratic Republic of the Congo (the)': 'COD',\n",
    "    'Lao People\\'s Democratic Republic (the)': 'LAO',\n",
    "    'State of Palestine': 'PSE',\n",
    "    'Niger (the)': 'NER',\n",
    "    'Philippines (the)': 'PHL',\n",
    "    'Bolivia (Plurinational State of)': 'BOL',\n",
    "    'Lao PDR': 'LAO',\n",
    "    'Timor Leste': 'TLS',\n",
    "    'Congo DR': 'COD',\n",
    "    'Bosnia-Herzegovina': 'BIH',\n",
    "    'Africa, Regional': 'AFR',\n",
    "    'St. Kitts And Nevis': 'KNA',\n",
    "    'St. Lucia': 'LCA',\n",
    "    'St. Vincent and Grenadines': 'VCT',\n",
    "    'Korea DPR': 'PRK',\n",
    "    'Regional, Timor Leste': 'Regional, Timor-Leste',\n",
    "    'Regional, Bosnia-Herzegovina': 'Regional, Bosnia and Herzegovina',\n",
    "    'Regional, Micronesia': 'Regional, Micronesia (Federated States of)',\n",
    "    'Regional, Cote d\\'Ivoire': 'Regional, CIV',\n",
    "    'Regional, St. Lucia': 'Regional, Saint Lucia',\n",
    "    'Cote d\\'Ivoire, Global': 'CIV, Global',\n",
    "    'Turkiye': 'TUR',\n",
    "    'Turkey': 'TUR',\n",
    "    'Central African Republic': 'CAF',\n",
    "    'Central African Republic (the)': 'CAF',\n",
    "    'CAR': 'CAF',\n",
    "    'Dominican Republic': 'DOM',\n",
    "    'Dominican Republic (the)': 'DOM',\n",
    "    'Republica Dominicana': 'DOM',\n",
    "    'Eswatini': 'SWZ',\n",
    "    'Swaziland': 'SWZ'\n",
    "}\n",
    "\n",
    "# Normalize country names\n",
    "def normalize_country_name(country_name):\n",
    "    # Handle prefixes like 'Regional,'\n",
    "    prefixes = ['Regional,', 'Global,']\n",
    "    for prefix in prefixes:\n",
    "        if country_name.startswith(prefix):\n",
    "            base_name = country_name[len(prefix):].strip()\n",
    "            normalized_base_name = country_normalization_mapping.get(base_name, base_name)\n",
    "            return f'{prefix} {normalized_base_name}'\n",
    "    \n",
    "    # Handle multiple countries\n",
    "    if ',' in country_name:\n",
    "        countries = country_name.split(',')\n",
    "        normalized_countries = [country_normalization_mapping.get(country.strip(), country.strip()) for country in countries]\n",
    "        return ', '.join(normalized_countries)\n",
    "    \n",
    "    # Single country\n",
    "    normalized_name = country_normalization_mapping.get(country_name, country_name)\n",
    "    return unidecode(normalized_name)\n",
    "\n",
    "# Apply the normalization function to the 'country' column\n",
    "data['country'] = data['country'].apply(normalize_country_name)\n",
    "\n",
    "# Function to get ISO3 code\n",
    "def get_iso3(country_name):\n",
    "    if country_name in country_normalization_mapping:\n",
    "        return country_normalization_mapping[country_name]\n",
    "    try:\n",
    "        return pycountry.countries.lookup(country_name).alpha_3\n",
    "    except LookupError:\n",
    "        return 'Unspecified'\n",
    "\n",
    "# Handle multiple country names in a cell\n",
    "def convert_countries_to_iso3(countries):\n",
    "    if ',' in countries:\n",
    "        # Split the countries and convert each one\n",
    "        country_list = countries.split(',')\n",
    "        iso3_list = [get_iso3(country.strip()) for country in country_list]\n",
    "        return ', '.join([code for code in iso3_list if code])\n",
    "    else:\n",
    "        # Single country\n",
    "        return get_iso3(countries)\n",
    "\n",
    "data['country_iso3'] = data['country'].apply(convert_countries_to_iso3)\n",
    "\n",
    "# Counting 'Unspecified' values in the 'country_iso3' column\n",
    "unspecified_count = (data['country_iso3'] == 'Unspecified').sum()\n",
    "print(\"Count of 'Unspecified' in 'country_iso3':\", unspecified_count)\n",
    "\n",
    "# Function to extract country names and convert to ISO3\n",
    "def extract_countries_to_iso3(title):\n",
    "    # Patterns to extract country names\n",
    "    pattern = r'\\b(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b'\n",
    "    \n",
    "    matches = re.findall(pattern, title)\n",
    "    \n",
    "    # Filter out words that are not country names\n",
    "    country_names = []\n",
    "    for name in matches:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(name)\n",
    "            country_names.append(country.alpha_3)\n",
    "        except LookupError:\n",
    "            continue\n",
    "    \n",
    "    # Join the ISO3 codes as a comma-separated string\n",
    "    return ', '.join(country_names) if country_names else 'Unspecified'\n",
    "\n",
    "filtered_df = data[data['country_iso3'] == 'Unspecified']\n",
    "# Apply the function to the entire DataFrame\n",
    "filtered_df['country_iso3'] = filtered_df['title'].apply(extract_countries_to_iso3)\n",
    "\n",
    "# Merge filtered_df back to the original data DataFrame\n",
    "data.update(filtered_df[['country_iso3']])\n",
    "\n",
    "# Counting 'Unspecified' values in the 'country_iso3' column again\n",
    "unspecified_count = (data['country_iso3'] == 'Unspecified').sum()\n",
    "print(\"Count of 'Unspecified' in 'country_iso3':\", unspecified_count)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "data.to_csv(\"D:\\\\data\\\\CGIAR\\\\UNFCCC\\\\master.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "# Check for the files\n",
    "folder_path = \"D:\\\\data\\\\CGIAR\\\\UNFCCC\"\n",
    "af_df = pd.read_csv(os.path.join(folder_path, \"AF_joined_projects_20240517.csv\"), encoding='utf-8')\n",
    "gcf_df = pd.read_csv(os.path.join(folder_path, \"GCF_joined_projects_20240517.csv\"), encoding='utf-8')\n",
    "gef_df = pd.read_csv(os.path.join(folder_path, \"GEF_joined_projects_20240517.csv\"), encoding='utf-8')\n",
    "\n",
    "print(f\"Total projects from the Adaptation Fund are {len(af_df)}. On the website, there are 149. Missing or not in dataset: {149 - len(af_df)}.\")\n",
    "print(f\"Total projects from the GCF are {len(gcf_df)}. On the website, there are 253. Missing or not in dataset: {253 - len(gcf_df)}.\")\n",
    "print(f\"Total projects from the GEF are {len(gef_df)}. On the website, there are 2402. Missing or not in dataset: {2402 - len(gef_df)}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode master file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pycountry\n",
    "\n",
    "# URL of the GitHub raw file\n",
    "GITHUB_URL = 'https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv'\n",
    "\n",
    "# Load the CSV file directly from GitHub\n",
    "regional_data = pd.read_csv(GITHUB_URL)\n",
    "\n",
    "# Load the master CSV file\n",
    "FILE_PATH = r\"D:\\data\\CGIAR\\UNFCCC\\master.csv\"\n",
    "data = pd.read_csv(FILE_PATH, encoding='utf-8')\n",
    "\n",
    "# Ensure 'Unspecified' entries are excluded\n",
    "data = data[data['country_iso3'].str.strip().str.lower() != 'unspecified']\n",
    "\n",
    "# Remove commas from 'grant_amount' and convert to numeric (float)\n",
    "data['grant_amount'] = data['grant_amount'].str.replace(',', '', regex=False).astype(float)\n",
    "\n",
    "# Function to normalize country names to ISO3 codes\n",
    "def normalize_country_name(country_name):\n",
    "    try:\n",
    "        return pycountry.countries.lookup(country_name).alpha_3\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "# Function to divide regional multinational projects\n",
    "def divide_multinational_projects(row):\n",
    "    countries = row['country'].split(', ')\n",
    "    country_iso3_list = row['country_iso3'].split(', ')\n",
    "    grant_amount = row['grant_amount'] / len(countries)\n",
    "    rows = []\n",
    "    for country, iso3 in zip(countries, country_iso3_list):\n",
    "        new_row = row.copy()\n",
    "        new_row['country'] = country\n",
    "        new_row['country_iso3'] = iso3\n",
    "        new_row['grant_amount'] = grant_amount\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "\n",
    "# Apply the function to create individual rows for each country in multinational projects\n",
    "expanded_data = data.apply(lambda row: divide_multinational_projects(row), axis=1)\n",
    "expanded_data = [item for sublist in expanded_data for item in sublist]\n",
    "expanded_data = pd.DataFrame(expanded_data)\n",
    "\n",
    "# Conflict-affected and Fragile countries from World Bank\n",
    "CONFLICT_COUNTRIES = [\n",
    "    'AFG', 'BFA', 'CMR', 'CAF', 'COD', \n",
    "    'ETH', 'IRQ', 'MLI', 'MOZ', 'MMR', 'NER', 'NGA', 'SOM', \n",
    "    'SSD', 'SDN', 'SYR', 'UKR', 'PSE', 'YEM'\n",
    "]\n",
    "\n",
    "FRAGILE_COUNTRIES = [\n",
    "    'BDI', 'TCD', 'COM', 'COG', 'ERI', 'GNB', 'HTI', \n",
    "    'KIR', 'XKX', 'LBN', 'LBY', 'MHL', 'FSM', \n",
    "    'PNG', 'STP', 'SLB', 'TLS', 'TUV', \n",
    "    'VEN', 'ZWE'\n",
    "]\n",
    "\n",
    "conflict_iso3 = [normalize_country_name(country) for country in CONFLICT_COUNTRIES]\n",
    "fragile_iso3 = [normalize_country_name(country) for country in FRAGILE_COUNTRIES]\n",
    "\n",
    "# Remove None values\n",
    "conflict_iso3 = [iso3 for iso3 in conflict_iso3 if iso3]\n",
    "fragile_iso3 = [iso3 for iso3 in fragile_iso3 if iso3]\n",
    "\n",
    "# Add 'FCS', 'FCS_conflict', and 'FCS_fragile' columns\n",
    "expanded_data['FCS'] = expanded_data['country_iso3'].isin(conflict_iso3 + fragile_iso3)\n",
    "expanded_data['FCS_conflict'] = expanded_data['country_iso3'].isin(conflict_iso3)\n",
    "expanded_data['FCS_fragile'] = expanded_data['country_iso3'].isin(fragile_iso3)\n",
    "\n",
    "# Map countries to their sub-regions using the regional data\n",
    "regional_data_mapping = dict(zip(regional_data['alpha-3'], regional_data['sub-region']))\n",
    "expanded_data['sub-region'] = expanded_data['country_iso3'].map(regional_data_mapping)\n",
    "\n",
    "# Exclude rows with 'country_iso3' as 'Unspecified'\n",
    "expanded_data = expanded_data[expanded_data['country_iso3'] != 'Unspecified']\n",
    "\n",
    "# Save the expanded data to a new CSV file\n",
    "EXPANDED_FILE_PATH = r\"D:\\data\\CGIAR\\UNFCCC\\exploded_master.csv\"\n",
    "expanded_data.to_csv(EXPANDED_FILE_PATH, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd master file pop aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_iso3</th>\n",
       "      <th>grant_amount</th>\n",
       "      <th>population</th>\n",
       "      <th>grant_per_capita</th>\n",
       "      <th>FCS</th>\n",
       "      <th>FCS_conflict</th>\n",
       "      <th>FCS_fragile</th>\n",
       "      <th>AF</th>\n",
       "      <th>GCF</th>\n",
       "      <th>GEF</th>\n",
       "      <th>grant_per_capita_AF</th>\n",
       "      <th>grant_per_capita_GCF</th>\n",
       "      <th>grant_per_capita_GEF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>7.523781e+07</td>\n",
       "      <td>41128771</td>\n",
       "      <td>1.829323</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.719884e+07</td>\n",
       "      <td>5.803897e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.418171</td>\n",
       "      <td>1.411153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGO</td>\n",
       "      <td>8.828583e+07</td>\n",
       "      <td>35588987</td>\n",
       "      <td>2.480706</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11941038.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.634479e+07</td>\n",
       "      <td>0.335526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALB</td>\n",
       "      <td>6.346315e+07</td>\n",
       "      <td>2777689</td>\n",
       "      <td>22.847464</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9927750.0</td>\n",
       "      <td>3.578221e+07</td>\n",
       "      <td>1.775319e+07</td>\n",
       "      <td>3.574104</td>\n",
       "      <td>12.882006</td>\n",
       "      <td>6.391354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARG</td>\n",
       "      <td>3.632480e+08</td>\n",
       "      <td>46234830</td>\n",
       "      <td>7.856587</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23936813.0</td>\n",
       "      <td>2.081240e+08</td>\n",
       "      <td>1.311871e+08</td>\n",
       "      <td>0.517723</td>\n",
       "      <td>4.501456</td>\n",
       "      <td>2.837409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARM</td>\n",
       "      <td>1.770356e+08</td>\n",
       "      <td>2780469</td>\n",
       "      <td>63.67114</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3941100.0</td>\n",
       "      <td>1.393975e+08</td>\n",
       "      <td>3.369706e+07</td>\n",
       "      <td>1.417423</td>\n",
       "      <td>50.134519</td>\n",
       "      <td>12.119198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_iso3  grant_amount population grant_per_capita    FCS  FCS_conflict  \\\n",
       "0          AFG  7.523781e+07   41128771         1.829323   True          True   \n",
       "1          AGO  8.828583e+07   35588987         2.480706  False         False   \n",
       "2          ALB  6.346315e+07    2777689        22.847464  False         False   \n",
       "3          ARG  3.632480e+08   46234830         7.856587  False         False   \n",
       "4          ARM  1.770356e+08    2780469         63.67114  False         False   \n",
       "\n",
       "   FCS_fragile          AF           GCF           GEF grant_per_capita_AF  \\\n",
       "0        False         0.0  1.719884e+07  5.803897e+07                 0.0   \n",
       "1        False  11941038.0  0.000000e+00  7.634479e+07            0.335526   \n",
       "2        False   9927750.0  3.578221e+07  1.775319e+07            3.574104   \n",
       "3        False  23936813.0  2.081240e+08  1.311871e+08            0.517723   \n",
       "4        False   3941100.0  1.393975e+08  3.369706e+07            1.417423   \n",
       "\n",
       "  grant_per_capita_GCF grant_per_capita_GEF  \n",
       "0             0.418171             1.411153  \n",
       "1                  0.0              2.14518  \n",
       "2            12.882006             6.391354  \n",
       "3             4.501456             2.837409  \n",
       "4            50.134519            12.119198  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pycountry\n",
    "import requests\n",
    "\n",
    "# Load the exploded master file\n",
    "FILE_PATH = r'D:\\data\\CGIAR\\UNFCCC\\exploded_master.csv'\n",
    "data = pd.read_csv(FILE_PATH, encoding='utf-8')\n",
    "\n",
    "# Group by country and sum the total grant amount per country\n",
    "grouped_data = data.groupby('country_iso3')['grant_amount'].sum().reset_index()\n",
    "\n",
    "# Function to get population data from World Bank API\n",
    "def get_population_data(iso3):\n",
    "    url = f\"https://api.worldbank.org/v2/country/{iso3}/indicator/SP.POP.TOTL?date=2022&format=json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if len(data) > 1 and isinstance(data[1], list) and len(data[1]) > 0:\n",
    "            return data[1][0].get('value', 'Unknown')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"HTTP error: {e}\")\n",
    "    except (IndexError, KeyError, TypeError) as e:\n",
    "        print(f\"Data error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return 'Unknown'\n",
    "\n",
    "# Add population data to the grouped data\n",
    "grouped_data['population'] = grouped_data['country_iso3'].apply(get_population_data)\n",
    "\n",
    "# Calculate per capita grant amount\n",
    "grouped_data['grant_per_capita'] = grouped_data.apply(\n",
    "    lambda row: row['grant_amount'] / row['population'] if row['population'] != 'Unknown' else 'Unknown', axis=1\n",
    ")\n",
    "\n",
    "# Group by country and organization, and sum the grant amount per country and organization\n",
    "org_grouped_data = data.groupby(['country_iso3', 'org'])['grant_amount'].sum().unstack(fill_value=0).reset_index()\n",
    "\n",
    "# Calculate per capita grant amount for each organization\n",
    "for org in org_grouped_data.columns[1:]:\n",
    "    org_grouped_data[f'grant_per_capita_{org}'] = org_grouped_data.apply(\n",
    "        lambda row: row[org] / grouped_data.loc[grouped_data['country_iso3'] == row['country_iso3'], 'population'].values[0]\n",
    "        if grouped_data.loc[grouped_data['country_iso3'] == row['country_iso3'], 'population'].values[0] != 'Unknown'\n",
    "        else 'Unknown', axis=1\n",
    "    )\n",
    "\n",
    "# Normalize country names\n",
    "def normalize_country_name(country_name):\n",
    "    try:\n",
    "        return pycountry.countries.lookup(country_name).alpha_3\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "# Assuming these variables are defined earlier in the code and not repeated\n",
    "CONFLICT_COUNTRIES = [\n",
    "    'AFG', 'BFA', 'CMR', 'CAF', 'COD', \n",
    "    'ETH', 'IRQ', 'MLI', 'MOZ', 'MMR', 'NER', 'NGA', 'SOM', \n",
    "    'SSD', 'SDN', 'SYR', 'UKR', 'PSE', 'YEM'\n",
    "]\n",
    "\n",
    "FRAGILE_COUNTRIES = [\n",
    "    'BDI', 'TCD', 'COM', 'COG', 'ERI', 'GNB', 'HTI', \n",
    "    'KIR', 'XKX', 'LBN', 'LBY', 'MHL', 'FSM', \n",
    "    'PNG', 'STP', 'SLB', 'TLS', 'TUV', \n",
    "    'VEN', 'ZWE'\n",
    "]\n",
    "\n",
    "# Normalize country names for conflict and fragile lists\n",
    "conflict_iso3 = [normalize_country_name(country) for country in CONFLICT_COUNTRIES]\n",
    "fragile_iso3 = [normalize_country_name(country) for country in FRAGILE_COUNTRIES]\n",
    "\n",
    "# Remove None values\n",
    "conflict_iso3 = [iso3 for iso3 in conflict_iso3 if iso3]\n",
    "fragile_iso3 = [iso3 for iso3 in fragile_iso3 if iso3]\n",
    "\n",
    "# Add 'FCS', 'FCS_conflict', and 'FCS_fragile' columns to the grouped data\n",
    "grouped_data['FCS'] = grouped_data['country_iso3'].isin(conflict_iso3 + fragile_iso3)\n",
    "grouped_data['FCS_conflict'] = grouped_data['country_iso3'].isin(conflict_iso3)\n",
    "grouped_data['FCS_fragile'] = grouped_data['country_iso3'].isin(fragile_iso3)\n",
    "\n",
    "# Merge the org grouped data with the main grouped data\n",
    "final_data = pd.merge(grouped_data, org_grouped_data, on='country_iso3')\n",
    "\n",
    "# Save the aggregated data to a new CSV file\n",
    "AGGREGATED_FILE_PATH = r\"D:\\data\\CGIAR\\UNFCCC\\aggregated_master.csv\"\n",
    "final_data.to_csv(AGGREGATED_FILE_PATH, index=False, encoding='utf-8')\n",
    "\n",
    "final_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison with GCF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country_iso3  grant_amount population grant_per_capita    FCS  FCS_conflict  \\\n",
      "0          AFG  7.523781e+07   41128771         1.829323   True          True   \n",
      "1          AGO  8.828583e+07   35588987         2.480706  False         False   \n",
      "2          ALB  6.346315e+07    2777689        22.847464  False         False   \n",
      "3          ARG  3.632480e+08   46234830         7.856587  False         False   \n",
      "4          ARM  1.770356e+08    2780469         63.67114  False         False   \n",
      "\n",
      "   FCS_fragile          AF           GCF           GEF grant_per_capita_AF  \\\n",
      "0        False         0.0  1.719884e+07  5.803897e+07                 0.0   \n",
      "1        False  11941038.0  0.000000e+00  7.634479e+07            0.335526   \n",
      "2        False   9927750.0  3.578221e+07  1.775319e+07            3.574104   \n",
      "3        False  23936813.0  2.081240e+08  1.311871e+08            0.517723   \n",
      "4        False   3941100.0  1.393975e+08  3.369706e+07            1.417423   \n",
      "\n",
      "  grant_per_capita_GCF grant_per_capita_GEF ISO3  FA Financing $  \\\n",
      "0             0.418171             1.411153  AFG    1.719884e+07   \n",
      "1                  0.0              2.14518  AGO             NaN   \n",
      "2            12.882006             6.391354  ALB    2.884957e+07   \n",
      "3             4.501456             2.837409  ARG    2.086892e+08   \n",
      "4            50.134519            12.119198  ARM    1.413978e+08   \n",
      "\n",
      "   error_percentage  \n",
      "0          0.000000  \n",
      "1               NaN  \n",
      "2         24.030283  \n",
      "3         -0.270798  \n",
      "4         -1.414684  \n"
     ]
    }
   ],
   "source": [
    "# Load the GCF export file\n",
    "gcf_file_path = r\"D:\\data\\CGIAR\\UNFCCC\\GCF-Export-countries-1716912441595.xlsx\" #use file exported with country level financing column FA Financing the other is for readiness grants. \n",
    "gcf_data = pd.read_excel(gcf_file_path)\n",
    "\n",
    "# Merge the GCF data with the final_data on country_iso3\n",
    "comparison_data = pd.merge(final_data, gcf_data[['ISO3', 'FA Financing $']], left_on='country_iso3', right_on='ISO3', how='inner')\n",
    "\n",
    "# Calculate the error percentage\n",
    "comparison_data['error_percentage'] = ((comparison_data['GCF'] - comparison_data['FA Financing $']) / comparison_data['FA Financing $']) * 100\n",
    "\n",
    "# Save the comparison data to a new CSV file\n",
    "comparison_file_path = r\"D:\\data\\CGIAR\\UNFCCC\\comparison_data.csv\"\n",
    "comparison_data.to_csv(comparison_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Display the comparison data\n",
    "print(comparison_data.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
